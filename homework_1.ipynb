{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Homework 1:  Retrieval Models: Boolean + Vector Space\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Thursday, February 2 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine. In particular, there are three main learning objectives: (i) the basics of tokenization (e.g. stemming, case-folding, etc.) and its effect on information retrieval; (ii) basics of index building and Boolean retrieval; and (iii) basics of the Vector Space model and ranked retrieval.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as lastname_firstinitial_hw#.ipynb. For example, my homework submission would be: caverlee_j_hw1.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Friday, February 5 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We provide the dataset, [southpark_scripts.zip](https://www.dropbox.com/s/6rzfsbn97s8vwof/southpark_scripts.zip), which includes scripts for episodes of the first twenty seasons of the TV show South Park. You will build an inverted index over these scripts where each episode should be treated as a single document. There are 277 episodes (documents) to index and search on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (20 points) Part 1: Parsing\n",
    "\n",
    "First, you should tokenize documents using **whitespaces and punctuations as delimiters** but do not remove stop words. Your parser needs to also provide the following two confgiuration options:\n",
    "* Case-folding\n",
    "* Stemming: use [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)\n",
    "\n",
    "Please note that you should stick to the stemming package listed above. Otherwise, given the same query, the results generated by your code can be different from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import operator\n",
    "# configuration options\n",
    "use_stemming = True # or false\n",
    "use_casefolding = True # or false\n",
    "stemmer = PorterStemmer()\n",
    "filesPath = \"../southpark_scripts/*.txt\"\n",
    "num_files = 0\n",
    "for fileName in glob.iglob(filesPath):\n",
    "    num_files += 1\n",
    "print num_files\n",
    "\n",
    "def tokenize(line, words):\n",
    "    temp_list = re.split('\\W+', line)\n",
    "    for word in temp_list:\n",
    "        if word == \"\":\n",
    "            continue\n",
    "        if use_casefolding == True:\n",
    "            word = word.lower()\n",
    "        if use_stemming == True:\n",
    "            word = stemmer.stem(word)\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17143\n"
     ]
    }
   ],
   "source": [
    "# Your parser function here. It will take the two option variables above as the parameters\n",
    "# add cells as needed to organize your code\n",
    "\n",
    "words = set()\n",
    "for fileName in glob.iglob(filesPath):\n",
    "    outFile = open(fileName, 'r')\n",
    "    temp_wordList = []\n",
    "    for line in outFile:\n",
    "        tokenize(line, temp_wordList)\n",
    "    for word in temp_wordList:\n",
    "        words.add(word)\n",
    "        \n",
    "print len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Once you have your parser working, you should report here the size of your dictionary under the four cases. That is, how many unique tokens do you have with stemming on and casefolding on? And so on. You should fill in the following\n",
    "\n",
    "* Stemming + Casefolding       = 17143\n",
    "* Stemming + No Casefolding    = 17368\n",
    "* No Stemming + Casefolding    = 23806\n",
    "* No Stemming + No Casefolding = 29550\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 2: Boolean Retrieval\n",
    "\n",
    "In this part you build an inverted index to support Boolean retrieval. We only require your index to  support AND queries. In other words, your index does not have to support OR, NOT, or parentheses. Also, we do not explicitly expect to see AND in queries, e.g., when we query **great again**, your search engine should treat it as **great** AND **again**.\n",
    "\n",
    "Example queries:\n",
    "* Rednecks\n",
    "* Troll Trace\n",
    "* Respect my authority\n",
    "* Respect my authoritah\n",
    "* Respected my authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# build the index here\n",
    "# add cells as needed to organize your code\n",
    "dict = defaultdict(list)\n",
    "for fileName in glob.iglob(filesPath):\n",
    "    outFile = open(fileName, 'r')\n",
    "    words = []\n",
    "    for line in outFile:\n",
    "        tokenize(line, words)\n",
    "    for word in words:\n",
    "        dict[word].append(fileName)\n",
    "print \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Search:yellow\n",
      "['yellow']\n",
      "1312\n",
      "1613\n",
      "0715\n",
      "1401\n",
      "0108\n",
      "0408\n",
      "1105\n",
      "2005\n",
      "1409\n",
      "1011\n",
      "1206\n",
      "1604\n",
      "0303\n",
      "1207\n",
      "0205\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Boolean Search:')\n",
    "# search for the input using your index and print out ids of matching documents\n",
    "words = []\n",
    "tokenize(search_text, words)\n",
    "print words\n",
    "results = set()\n",
    "for fileName in glob.iglob(filesPath):\n",
    "    results.add(fileName)\n",
    "for word in words:\n",
    "    if word in dict:\n",
    "        results = results.intersection(dict[word])\n",
    "    else:\n",
    "        results.clear()\n",
    "        break\n",
    "\n",
    "for fileName in results:\n",
    "    #print fileName\n",
    "    print (re.findall('([0-9]+)', fileName))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "When we use stemming and casefolding, is the result different from the result when we do not use them? Do you find cases where you prefer stemming? Or not? Or cases where you prefer casefolding? Or Not? Write down your observations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your observations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Phrase Queries\n",
    "\n",
    "Your search engine needs to also (optionally) support phrase queries of arbitrary length. Use quotes in a query to tell your search engine this is a phrase query. Again, we don't explicitly type AND in queries and never use OR, NOT, or parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 3: Ranked Retrieval\n",
    "\n",
    "In this part, your job is to support queries over an index that you build. This time you will use the vector space model plus cosine similarity to rank documents.\n",
    "\n",
    "**TFIDF:** For the document vectors, use the standard TFIDF scores. That is, use the log-weighted term frequency $1+log(tf)$; and the log-weighted inverse document frequency $log(\\frac{N}{df})$. For the query vector, use simple weights (the raw term frequency). For example:\n",
    "* query: troll $\\rightarrow$ (1)\n",
    "* query: troll trace $\\rightarrow$ (1, 1)\n",
    "\n",
    "**Output:**\n",
    "For a given query, you should rank all the 277 documents but you only need to output the top-5 documents (i.e. document ids) plus the cosine score of each of these documents. For example:\n",
    "\n",
    "* result1 - score1\n",
    "* result2 - score2\n",
    "* result3 - score3\n",
    "* result4 - score4\n",
    "* result5 - score5\n",
    "\n",
    "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the vector space index here\n",
    "# add cells as needed to organize your code\n",
    "\n",
    "def magnitude(u):\n",
    "    return math.sqrt(sum(u[i]*u[i] for i in range(len(u))))\n",
    "\n",
    "def magnitude_map(u):\n",
    "    return math.sqrt(sum(u[i]*u[i] for i in u))\n",
    "\n",
    "def dot(u, v):\n",
    "    return sum(u[i]*v[i] for i in range(len(u)))\n",
    "\n",
    "def normalize(v):\n",
    "    vmag = magnitude(v)\n",
    "    if vmag == 0:\n",
    "        return v\n",
    "    return [ float(v[i])/vmag  for i in range(len(v)) ]\n",
    "\n",
    "def normalize_map(v):\n",
    "    vmag = magnitude_map(v)\n",
    "    if vmag == 0:\n",
    "        return v\n",
    "    for i in v:\n",
    "        v[i] = float(v[i])/vmag \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Search:King of Westeros\n",
      "('../southpark_scripts/1607.txt', 0.11141491456769549)\n",
      "('../southpark_scripts/1708.txt', 0.03297928740186223)\n",
      "('../southpark_scripts/0502.txt', 0.02648499578638105)\n",
      "('../southpark_scripts/0614.txt', 0.025890594686245796)\n",
      "('../southpark_scripts/1704.txt', 0.02476347600982693)\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Ranked Search:')\n",
    "# search for the input and print the top 5 document ids along with their associated cosine scores.\n",
    "query_words = []\n",
    "words_set = set()\n",
    "doc_vec_dict = defaultdict(list)\n",
    "query_vec = []\n",
    "results = defaultdict(int)\n",
    "\n",
    "# query tokenization\n",
    "tokenize(search_text, query_words)\n",
    "words_set = set(query_words)\n",
    "visited = []\n",
    "for word in query_words:\n",
    "    if word in visited:\n",
    "        continue\n",
    "    query_vec.append(query_words.count(word))\n",
    "    visited.append(word)\n",
    "query_vec = normalize(query_vec)\n",
    "#print query_vec\n",
    "\n",
    "for fileName in glob.iglob(filesPath):\n",
    "    #doc_vec = [0 for x in xrange(len(words_set))]\n",
    "    doc_vec = {}\n",
    "    for word in dict:\n",
    "        #Term freqency\n",
    "        num_relevant_docs = len(set(dict[word]))\n",
    "        idf = math.log10((float(num_files)/num_relevant_docs))\n",
    "        #print \"idf\" + fileName + \" \" + str(num_relevant_docs)\n",
    "        \n",
    "        #print idf\n",
    "        tf = 0\n",
    "        if dict[word].count(fileName) > 0:\n",
    "            #print fileName + \" \" + word + \" \" + str(dict[word].count(fileName))\n",
    "            tf = 1 + math.log10(dict[word].count(fileName))\n",
    "        #print \"tf\" + fileName + \" \" + word + \" \" + str(tf)\n",
    "        \n",
    "        doc_vec[word] = tf*idf\n",
    "    #print doc_vec\n",
    "    doc_vec = normalize_map(doc_vec)\n",
    "    relevant_doc_vec = []\n",
    "    visited = []\n",
    "    for word in query_words:\n",
    "        if word in visited:\n",
    "            continue\n",
    "        relevant_doc_vec.append(doc_vec[word])\n",
    "        visited.append(word)\n",
    "        \n",
    "    results[fileName] = dot(relevant_doc_vec, query_vec)\n",
    "    \n",
    "sorted_x = sorted(results.items(), key=operator.itemgetter(1), reverse=True)\n",
    "count = 0\n",
    "for item in sorted_x:\n",
    "    if count == 5:\n",
    "        break\n",
    "    print item\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grader will randomly pick 5-10 queries to test your program. You are welcome to discuss the results returned by your search engine with others on Piazza."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
