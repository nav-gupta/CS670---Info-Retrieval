{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Homework 3:  Classification Cook-off: Naive Bayes vs Rocchio (plus a little bit of recommenders)\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Wednesday, March 29 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Hands-on practice building and evaluating classifiers.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as YOUR_UIN_hw3.ipynb. Submit this notebook via eCampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Saturday April 1 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Yelp review data\n",
    "\n",
    "In this assignment, given a Yelp review, your task is to implement two classifiers to predict if the business category of this review is \"food-relevant\" or not, **only based on the review text**. The data is from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge).\n",
    "\n",
    "## Build the training data\n",
    "\n",
    "First, you will need to download this data file as your training data: [training_data.json](https://drive.google.com/open?id=0B_13wIEAmbQMdzBVTndwenoxQlk) \n",
    "\n",
    "The training data file includes 40,000 Yelp reviews. Each line is a json-encoded review, and **you should only focus on the \"text\" field**. As same as in homework 1, you should tokenize the review text by using the regular expression \"\\W+\" (we discussed it in [this Piazza post](https://piazza.com/class/ixkk1fy863r1vs?cid=29). Do NOT remove stop words. **Do casefolding but no stemming**.\n",
    "\n",
    "The label (class) information of each review is in the \"label\" field. It is **either \"Food-relevant\" or \"Food-irrelevant\"**.\n",
    "\n",
    "## Testing data\n",
    "\n",
    "We provide 100 yelp reviews here: [testing_data.json](https://drive.google.com/open?id=0B_13wIEAmbQMbXdyTkhrZDN4Wms). The testing data file has the same format as the training data file. Again, you can get the label informaiton in the \"label\" field. Only use it when you evalute your classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Naive Bayes classifier [35 points]\n",
    "\n",
    "In this part, you will implement a Naive Bayes classifier, which outputs the probabilities that a given review belongs to each class.\n",
    "\n",
    "Use a mixture model that mixes the probability from the document with the general collection frequency of the word. **You should use lambda = 0.7**. Be careful about the decimal rounding since multiplying many probabilities can generate a tiny value. We will not grade on the exact probability value, so feel free to change to logorithm summation (it's not required, though). If the tie case happens, **always go to the \"Food-irrelevant\" side**.\n",
    "\n",
    "### What to report\n",
    "\n",
    "* For the entire testing dataset, report the overall accuracy.\n",
    "* For the class \"Food-relevant\", report the precision and recall.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall.\n",
    "\n",
    "We will also grade on the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vocab size :  53138\n"
     ]
    }
   ],
   "source": [
    "# Build the naive bayes classifier\n",
    "# Insert as many cells as you want\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "lamda = 0.7\n",
    "klassesCount = defaultdict(lambda: 0.0)\n",
    "totalDocs = 0\n",
    "vocab = set()\n",
    "klassTotalWordCount = defaultdict(lambda: 0.0)\n",
    "klassWordCount = defaultdict(lambda: 0.0)\n",
    "klasses = [\"Food-relevant\", \"Food-irrelevant\"]\n",
    "\n",
    "def add_review(review, klass):\n",
    "    global klassesCount, totalDocs, vocab, klassTotalWordCount, klassWordCount\n",
    "    totalDocs += 1\n",
    "    klassesCount[klass] += 1\n",
    "    temp_list = re.split('\\W+', review)\n",
    "    for word in temp_list:\n",
    "        word = word.lower()\n",
    "        vocab.add(word)\n",
    "        klassTotalWordCount[klass] += 1\n",
    "        klassWordCount[(klass, word)] += 1\n",
    "\n",
    "def parse_json():\n",
    "    with open('training_data.json') as data_file:\n",
    "        for line in data_file:\n",
    "            data = json.loads(line)\n",
    "            klass = data[\"label\"]\n",
    "            review = data[\"text\"]\n",
    "            add_review(review, klass)\n",
    "\n",
    "parse_json()\n",
    "print \"vocab size : \", len(vocab)\n",
    "            \n",
    "def nb_classify(review):\n",
    "    klass_prob = {}\n",
    "    temp_list = re.split('\\W+', review)\n",
    "    for klass in klasses:\n",
    "        klass_prob[klass] = math.log(((0.0 + klassesCount[klass]) / totalDocs), 10)\n",
    "        for word in temp_list:\n",
    "            if word not in vocab:\n",
    "                continue\n",
    "            num2 = num1 = klassWordCount[(klass, word)]\n",
    "            den2 = den1 = klassTotalWordCount[klass]\n",
    "            if klass == klasses[0]:\n",
    "                num2 += classWordCount[(klasses[1], word)]\n",
    "                den2 += classTotalWordCount[klasses[1]]\n",
    "            else:\n",
    "                num2 += classWordCount[(klasses[0], word)]\n",
    "                den2 += classTotalWordCount[klasses[0]]\n",
    "            klass_prob[klass] += math.log((lamda*((0.0 + num1) / den1) + (1 - lamda)*((0.0 + num2) / den2)), 10)\n",
    "    return klass_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-85.9605398301   -84.6262793803   Food-relevant   Food-relevant\n",
      "-44.4595026398   -41.5561409261   Food-relevant   Food-relevant\n",
      "-336.043129789   -325.379015812   Food-relevant   Food-relevant\n",
      "-19.9468663671   -19.1756078399   Food-relevant   Food-relevant\n",
      "-143.89214258   -138.680106306   Food-relevant   Food-relevant\n",
      "-159.981486473   -161.390028251   Food-irrelevant   Food-relevant\n",
      "-238.641632016   -225.304594928   Food-relevant   Food-relevant\n",
      "-281.695925909   -277.628365275   Food-relevant   Food-relevant\n",
      "-50.9041378683   -48.5960726241   Food-relevant   Food-relevant\n",
      "-20.3582462189   -19.8082169512   Food-relevant   Food-relevant\n",
      "-108.001180662   -106.180150799   Food-relevant   Food-relevant\n",
      "-229.617521629   -227.751351272   Food-relevant   Food-relevant\n",
      "-54.302039354   -52.3646814337   Food-relevant   Food-relevant\n",
      "-125.107836799   -120.939639341   Food-relevant   Food-relevant\n",
      "-65.1094196263   -60.4150487226   Food-relevant   Food-relevant\n",
      "-355.802774413   -348.047155238   Food-relevant   Food-relevant\n",
      "-1357.17015688   -1318.46915003   Food-relevant   Food-relevant\n",
      "-138.061999888   -139.556048534   Food-irrelevant   Food-irrelevant\n",
      "-342.496323344   -353.262748558   Food-irrelevant   Food-irrelevant\n",
      "-422.636577163   -430.037524205   Food-irrelevant   Food-irrelevant\n",
      "-229.110599371   -230.664744642   Food-irrelevant   Food-irrelevant\n",
      "-110.443221919   -111.984541959   Food-irrelevant   Food-irrelevant\n",
      "-595.522623131   -599.530922977   Food-irrelevant   Food-irrelevant\n",
      "-125.887857971   -123.200046952   Food-relevant   Food-irrelevant\n",
      "-151.868032484   -155.388437293   Food-irrelevant   Food-irrelevant\n",
      "-322.154167731   -322.853416782   Food-irrelevant   Food-irrelevant\n",
      "-197.53085332   -198.404032418   Food-irrelevant   Food-irrelevant\n",
      "-246.999479112   -253.903133312   Food-irrelevant   Food-irrelevant\n",
      "-1459.0176771   -1483.17148183   Food-irrelevant   Food-irrelevant\n",
      "-1050.9627799   -1069.64966569   Food-irrelevant   Food-irrelevant\n",
      "-544.023882921   -554.894005157   Food-irrelevant   Food-irrelevant\n",
      "-237.603346407   -232.088879401   Food-relevant   Food-relevant\n",
      "-1206.17285241   -1180.79901002   Food-relevant   Food-relevant\n",
      "-591.731136198   -580.178422932   Food-relevant   Food-relevant\n",
      "-587.372790448   -575.473961722   Food-relevant   Food-relevant\n",
      "-50.5511543666   -47.4282208996   Food-relevant   Food-relevant\n",
      "-65.5262569422   -63.5070931992   Food-relevant   Food-relevant\n",
      "-92.8440619996   -89.105324464   Food-relevant   Food-relevant\n",
      "-173.394719327   -166.137158999   Food-relevant   Food-relevant\n",
      "-984.2398039   -970.310477016   Food-relevant   Food-relevant\n",
      "-71.9337351559   -68.095001052   Food-relevant   Food-relevant\n",
      "-569.184032734   -551.616269294   Food-relevant   Food-relevant\n",
      "-347.098090656   -323.55550859   Food-relevant   Food-relevant\n",
      "-608.110384583   -607.355013105   Food-relevant   Food-relevant\n",
      "-128.413764775   -125.673122482   Food-relevant   Food-relevant\n",
      "-176.15388482   -166.983169005   Food-relevant   Food-relevant\n",
      "-450.082437636   -435.047883717   Food-relevant   Food-relevant\n",
      "-364.533339968   -340.791115736   Food-relevant   Food-relevant\n",
      "-558.094280786   -556.589821144   Food-relevant   Food-relevant\n",
      "-215.984389917   -208.430102677   Food-relevant   Food-relevant\n",
      "-447.215453098   -434.187268348   Food-relevant   Food-relevant\n",
      "-181.968996983   -172.896785819   Food-relevant   Food-relevant\n",
      "-149.928745176   -146.894882243   Food-relevant   Food-relevant\n",
      "-119.360079651   -115.771782398   Food-relevant   Food-relevant\n",
      "-418.386737096   -408.176857502   Food-relevant   Food-relevant\n",
      "-318.235508125   -304.983982319   Food-relevant   Food-relevant\n",
      "-124.625006664   -122.313586783   Food-relevant   Food-relevant\n",
      "-100.163393779   -97.4124621684   Food-relevant   Food-relevant\n",
      "-59.0572767535   -54.0195296832   Food-relevant   Food-relevant\n",
      "-412.164440307   -401.995399105   Food-relevant   Food-relevant\n",
      "-632.037165451   -609.66205842   Food-relevant   Food-relevant\n",
      "-624.76469244   -617.600144614   Food-relevant   Food-relevant\n",
      "-1414.2264025   -1379.37125821   Food-relevant   Food-relevant\n",
      "-937.749016324   -917.476780834   Food-relevant   Food-relevant\n",
      "-468.057659595   -452.649574062   Food-relevant   Food-relevant\n",
      "-1329.76357129   -1292.09735512   Food-relevant   Food-relevant\n",
      "-210.324052517   -203.300116389   Food-relevant   Food-relevant\n",
      "-41.4204510954   -39.8090066968   Food-relevant   Food-relevant\n",
      "-750.035160144   -742.879614664   Food-relevant   Food-relevant\n",
      "-83.1977362067   -76.8533484836   Food-relevant   Food-relevant\n",
      "-69.8373419601   -71.9537638384   Food-irrelevant   Food-irrelevant\n",
      "-54.957327362   -54.4839345146   Food-relevant   Food-irrelevant\n",
      "-77.2732890681   -77.6702667269   Food-irrelevant   Food-irrelevant\n",
      "-873.763780356   -844.649736247   Food-relevant   Food-relevant\n",
      "-203.18185692   -195.583638477   Food-relevant   Food-relevant\n",
      "-210.297010068   -205.718510293   Food-relevant   Food-relevant\n",
      "-893.087549662   -877.877158739   Food-relevant   Food-relevant\n",
      "-30.1407006586   -31.5508915841   Food-irrelevant   Food-irrelevant\n",
      "-99.2048969336   -103.65629718   Food-irrelevant   Food-irrelevant\n",
      "-337.713252856   -349.097314421   Food-irrelevant   Food-irrelevant\n",
      "-276.524315602   -279.571701092   Food-irrelevant   Food-irrelevant\n",
      "-176.481039882   -177.691857192   Food-irrelevant   Food-irrelevant\n",
      "-433.444025237   -426.873672488   Food-relevant   Food-irrelevant\n",
      "-400.160291167   -400.954029735   Food-irrelevant   Food-irrelevant\n",
      "-995.277417421   -1009.11730167   Food-irrelevant   Food-irrelevant\n",
      "-81.9606992764   -81.1968223252   Food-relevant   Food-irrelevant\n",
      "-363.716537191   -362.470034271   Food-relevant   Food-irrelevant\n",
      "-354.156298373   -356.224217317   Food-irrelevant   Food-irrelevant\n",
      "-168.360510394   -168.680421712   Food-irrelevant   Food-irrelevant\n",
      "-40.9662525546   -40.4651009323   Food-relevant   Food-irrelevant\n",
      "-267.022626763   -265.834774295   Food-relevant   Food-irrelevant\n",
      "-449.916786944   -454.200218821   Food-irrelevant   Food-irrelevant\n",
      "-285.520964443   -274.355831549   Food-relevant   Food-relevant\n",
      "-73.4857053819   -71.8089847936   Food-relevant   Food-relevant\n",
      "-389.586871254   -379.717110888   Food-relevant   Food-relevant\n",
      "-366.639941509   -358.716179257   Food-relevant   Food-relevant\n",
      "-354.479068635   -354.984121439   Food-irrelevant   Food-relevant\n",
      "-488.147793626   -480.626710827   Food-relevant   Food-relevant\n",
      "-291.841905699   -290.613743316   Food-relevant   Food-relevant\n",
      "-185.437244308   -186.671288427   Food-irrelevant   Food-relevant\n",
      "accuracy :  0.9\n",
      "Food-relevant  : \n",
      "Precision :  0.902777777778\n",
      "Recall :  0.955882352941\n",
      "Food-irrelevant  : \n",
      "Precision :  0.892857142857\n",
      "Recall :  0.78125\n"
     ]
    }
   ],
   "source": [
    "# Apply your classifier on the test data. Report the results.\n",
    "# Insert as many cells as you want\n",
    "def test():\n",
    "    confusion_matrix = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    with open('testing_data.json') as data_file:\n",
    "        total_docs = 0.0\n",
    "        correct_docs = 0.0\n",
    "        incorrect_docs = {}\n",
    "        for line in data_file:\n",
    "            total_docs += 1\n",
    "            data = json.loads(line)\n",
    "            klass_prob = nb_classify(data[\"text\"])\n",
    "            predicted_class = None\n",
    "            max_prob = -sys.maxint -1\n",
    "            for klass in klasses:\n",
    "                temp_prob = klass_prob[klass]\n",
    "                if temp_prob > max_prob:\n",
    "                    max_prob = temp_prob\n",
    "                    predicted_class = klass\n",
    "            print klass_prob[classes[0]], \" \", klass_prob[classes[1]], \" \", predicted_class, \" \", data[\"label\"]\n",
    "            if predicted_class == data[\"label\"]:\n",
    "                correct_docs += 1\n",
    "                confusion_matrix[predicted_class][\"True\"] += 1\n",
    "            else:\n",
    "                incorrect_docs[total_docs] = (data[\"text\"], data[\"label\"])\n",
    "                confusion_matrix[predicted_class][\"False\"] += 1\n",
    "    print \"accuracy : \", correct_docs / total_docs\n",
    "    print klasses[0], \" : \"\n",
    "    print \"Precision : \", confusion_matrix[klasses[0]][\"True\"] / (confusion_matrix[klasses[0]][\"True\"] + confusion_matrix[klasses[0]][\"False\"])\n",
    "    print \"Recall : \", confusion_matrix[klasses[0]][\"True\"] / (confusion_matrix[klasses[0]][\"True\"] + confusion_matrix[klasses[1]][\"False\"])\n",
    "    \n",
    "    print klasses[1], \" : \"\n",
    "    print \"Precision : \", confusion_matrix[klasses[1]][\"True\"] / (confusion_matrix[klasses[1]][\"True\"] + confusion_matrix[klasses[1]][\"False\"])\n",
    "    print \"Recall : \", confusion_matrix[klasses[1]][\"True\"] / (confusion_matrix[klasses[1]][\"True\"] + confusion_matrix[klasses[0]][\"False\"])\n",
    "    '''\n",
    "    for key in incorrect_docs:\n",
    "        print key, \" \", incorrect_docs[key]\n",
    "        print\n",
    "    '''\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Rocchio classifier [35 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, your job is to implement a Rocchio classifier for \"food-relevant vs. food-irrelevant\". You need to aggregate all the reviews of each class, and find the center. **Use the normalized raw term frequency**.\n",
    "\n",
    "\n",
    "### What to report\n",
    "\n",
    "* For the entire testing dataset, report the overall accuracy.\n",
    "* For the class \"Food-relevant\", report the precision and recall.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall.\n",
    "\n",
    "We will also grade on the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magnitude(u):\n",
    "    return math.sqrt(sum(u[i]*u[i] for i in range(len(u))))\n",
    "\n",
    "def magnitude_map(u):\n",
    "    return math.sqrt(sum(u[i]*u[i] for i in u))\n",
    "\n",
    "def normalize(u):\n",
    "    mag = magnitude(u)\n",
    "    if mag == 0:\n",
    "        return u\n",
    "    return [float(u[i])/mag for i in range(len(u))]\n",
    "\n",
    "def normalize_map(u):\n",
    "    mag = magnitude_map(u)\n",
    "    if mag == 0:\n",
    "        return u\n",
    "    for i in u:\n",
    "        u[i] = float(u[i])/mag\n",
    "    return u\n",
    "\n",
    "def centroid(u, num_docs):\n",
    "    return [float(u[i])/num_docs for i in range(len(u))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the Rocchio classifier\n",
    "# Insert as many cells as you want\n",
    "klasses = [\"Food-relevant\", \"Food-irrelevant\"]\n",
    "klass_vec = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "doc_count = defaultdict(lambda: 0.0)\n",
    "vocab = set()\n",
    "\n",
    "def tokenize(review):\n",
    "    word_list = re.split('\\W+', review)\n",
    "    temp_vec = defaultdict(lambda: 0.0)\n",
    "    for word in word_list:\n",
    "        word = word.lower()\n",
    "        vocab.add(word)\n",
    "        temp_vec[word] += 1\n",
    "    return normalize_map(temp_vec)\n",
    "\n",
    "def rocchio_training():\n",
    "    with open(\"training_data.json\") as data_file:\n",
    "        for line in data_file:\n",
    "            data = json.loads(line)\n",
    "            label = data[\"label\"]\n",
    "            doc_count[label] += 1\n",
    "            review = data[\"text\"]\n",
    "            doc_vec = tokenize(review)\n",
    "            for word in doc_vec:\n",
    "                klass_vec[label][word] += doc_vec[word]\n",
    "\n",
    "def gen_centroid_vecs():\n",
    "    for klass in klasses:\n",
    "        num_docs = doc_count[klass]\n",
    "        for word in vocab:\n",
    "            klass_vec[klass][word] = float(klass_vec[klass][word]) / num_docs\n",
    "            \n",
    "def manhattan_dist(klass, doc_vec):\n",
    "    dist = 0.0\n",
    "    for word in vocab:\n",
    "        dist += abs(klass_vec[klass][word] - doc_vec[word])\n",
    "    return dist\n",
    "\n",
    "def euclidean_dist(klass, doc_vec):\n",
    "    dist = 0.0\n",
    "    for word in vocab:\n",
    "        dist += ((klass_vec[klass][word] - doc_vec[word])*(klass_vec[klass][word] - doc_vec[word]))\n",
    "    \n",
    "    return math.sqrt(dist)\n",
    "\n",
    "def rocchio_classify_manhattan(review):\n",
    "    doc_vec = tokenize(review)\n",
    "    min_dist = sys.maxint\n",
    "    predicted_klass = None\n",
    "    for klass in klasses:\n",
    "        '''\n",
    "        dist = 0.0\n",
    "        for word in vocab:\n",
    "            dist += abs(klass_vec[klass][word] - doc_vec[word])\n",
    "        '''\n",
    "        #dist = manhattan_dist(klass, doc_vec)\n",
    "        dist = euclidean_dist(klass, doc_vec)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            predicted_klass = klass\n",
    "    return predicted_klass\n",
    "\n",
    "\n",
    "rocchio_training()\n",
    "gen_centroid_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.67\n",
      "Food-relevant  : \n",
      "Precision :  0.818181818182\n",
      "Recall :  0.661764705882\n",
      "Food-irrelevant  : \n",
      "Precision :  0.488888888889\n",
      "Recall :  0.6875\n"
     ]
    }
   ],
   "source": [
    "# Apply your classifier on the test data. Report the results.\n",
    "# Insert as many cells as you want\n",
    "def test():\n",
    "    incorrect_docs = {}\n",
    "    total_docs = 0.0\n",
    "    confusion_matrix = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    with open(\"testing_data.json\") as data_file:\n",
    "        for line in data_file:\n",
    "            total_docs += 1.0\n",
    "            data = json.loads(line)\n",
    "            review = data[\"text\"]\n",
    "            predicted_klass = rocchio_classify_manhattan(review)\n",
    "            if predicted_klass != data[\"label\"]:\n",
    "                incorrect_docs[total_docs] = (data[\"text\"], data[\"label\"])\n",
    "                confusion_matrix[predicted_klass][\"False\"] += 1\n",
    "            else:\n",
    "                confusion_matrix[predicted_klass][\"True\"] += 1\n",
    "    \n",
    "    print \"accuracy : \", float(total_docs - len(incorrect_docs)) / total_docs\n",
    "    print klasses[0], \" : \"\n",
    "    print \"Precision : \", float(confusion_matrix[klasses[0]][\"True\"]) / (confusion_matrix[klasses[0]][\"True\"] + confusion_matrix[klasses[0]][\"False\"])\n",
    "    print \"Recall : \", float(confusion_matrix[klasses[0]][\"True\"]) / (confusion_matrix[klasses[0]][\"True\"] + confusion_matrix[klasses[1]][\"False\"])\n",
    "                                                                                       \n",
    "    print klasses[1], \" : \"\n",
    "    print \"Precision : \", float(confusion_matrix[klasses[1]][\"True\"]) / (confusion_matrix[klasses[1]][\"True\"] + confusion_matrix[klasses[1]][\"False\"])\n",
    "    print \"Recall : \", float(confusion_matrix[klasses[1]][\"True\"]) / (confusion_matrix[klasses[1]][\"True\"] + confusion_matrix[klasses[0]][\"False\"])  \n",
    "    '''\n",
    "    #print len(incorrect_docs), \" \", total_docs\n",
    "    for key in incorrect_docs:\n",
    "        #print key, \" : \", incorrect_docs[key]\n",
    "        print key\n",
    "    '''\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Naive Bayes vs. Rocchio [20 points]\n",
    "\n",
    "Which method gives the better results? In terms of what? How did you compare them? Can you explain why you observe what you do? Write 1-3 paragraphs below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Recommenders [10 points]\n",
    "\n",
    "Finally, since we've begun our discussion of recommenders, let's do a quick problem too:\n",
    "\n",
    "The table below is a utility matrix, representing the ratings, on a 1â€“5 star scale, of eight items, *a* through *h*, by three users *A*, *B*, and *C*. \n",
    "<pre>\n",
    "\n",
    "  | a  b  c  d  e  f  g  h\n",
    "--|-----------------------\n",
    "A | 4  5     5  1     3  2\n",
    "B |    3  4  3  1  2  1\n",
    "C | 2     1  3     4  5  3\n",
    "\n",
    "</pre>\n",
    "\n",
    "Compute the following from the data of this matrix.\n",
    "\n",
    "(a) Treating the utility matrix as boolean, compute the Jaccard distance between each pair of users.\n",
    "\n",
    "(b) Repeat Part (a), but use the cosine distance.\n",
    "\n",
    "(c) Treat ratings of 3, 4, and 5 as 1 and 1, 2, and blank as 0. Compute the Jaccard distance between each pair of users.\n",
    "\n",
    "(d) Repeat Part (c), but use the cosine distance.\n",
    "\n",
    "(e) Normalize the matrix by subtracting from each nonblank entry the average\n",
    "value for its user.\n",
    "\n",
    "(f) Using the normalized matrix from Part (e), compute the cosine distance\n",
    "between each pair of users.\n",
    "\n",
    "(g) Which of the approaches above seems most reasonable to you? Give a one or two sentence argument supporting your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your answer here:**\n",
    "\n",
    "(a)\n",
    "\n",
    "(b)\n",
    "\n",
    "(c)\n",
    "\n",
    "(d)\n",
    "\n",
    "(e)\n",
    "\n",
    "(f)\n",
    "\n",
    "(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
